# Example configuration file for ndict speech-to-text daemon
# Copy this to ~/.config/ndict/config.toml to customize

[audio]
# Audio device name (use "default" for system default)
device = "default"
# Sample rate in Hz (16kHz is recommended for Whisper)
sample_rate = 16000
# Number of samples per audio chunk (512 = ~32ms at 16kHz)
chunk_size = 512
# Audio gain multiplier (increase if microphone is too quiet)
gain = 1.0
# Number of audio channels (1 = mono, 2 = stereo)
# Whisper requires mono audio; stereo will be downsampled to mono
channels = 1

[vad]
# Threshold to START recording (audio level must be above this to begin)
# Higher values = less sensitive, won't start recording for quiet sounds
threshold_start = 0.02
# Threshold to STOP recording (audio level must drop below this to end)
# Lower than threshold_start to create hysteresis and prevent rapid toggling
# Typical: threshold_stop should be 50-80% of threshold_start
threshold_stop = 0.01
# Minimum silence duration in ms to confirm end of speech
min_silence_duration_ms = 1000
# Minimum speech duration in ms to consider it valid speech
min_speech_duration_ms = 250

[whisper]
# Optional: custom path to Whisper model file
# model_path = "/path/to/model.ggml"
# URL to download Whisper model from if not found locally
# HuggingFace URLs recommended
model_url = "https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-base.bin"
# Language code (e.g., "en", "es", "auto" for auto-detection)
language = "auto"
# Backend to use: cpu (default), gpu, cuda
# GPU backend on AMD/ROCm may have initialization issues.
# If GPU fails, daemon will automatically fall back to CPU.
# Uses whisper-rs from Codeberg (https://codeberg.org/tazz4843/whisper-rs)
backend = "cpu"
# Enable streaming transcription (true) or batch transcription (false)
# Streaming: transcribes continuously as you speak (lower latency)
# Batch: waits for silence before transcribing (higher accuracy)
streaming_mode = false
# Minimum audio samples required for Whisper transcription (18000 = ~1.125s at 16kHz)
# Audio shorter than this will be padded with silence before transcription
min_audio_samples = 18000
# Whisper sampling strategy: "greedy" (faster) or "beam" (slower, potentially more accurate)
sampling_strategy = "greedy"

[streaming]
# Streaming transcription settings (only used if whisper.streaming_mode = true)
# Audio chunk size in ms to send to Whisper (3000 = 3 seconds)
step_ms = 3000
# Total audio window length in ms for transcription context (10000 = 10 seconds)
length_ms = 10000
# Overlap between windows in ms (500 = 0.5 seconds)
# Higher overlap = better word boundaries but more processing
keep_ms = 500

[buffer]
# Audio buffer configuration for internal audio streaming
# Number of audio chunks to buffer (100 = ~3 seconds at 32ms/chunk)
# Higher values = more tolerance for processing delays, higher memory usage
broadcast_capacity = 100

[output]
# Typing mode: "instant" or "paste"
typing_mode = "instant"

[rate_limit]
# Command rate limiting to prevent flooding
# Maximum sustained rate of commands per second
commands_per_second = 10
# Maximum burst of commands (allows short bursts)
burst_capacity = 20
# Enable/disable rate limiting (true = enabled, false = disabled)
enabled = true

[timeouts]
# Whisper transcription timeout in seconds
# How long to wait for Whisper to finish transcribing audio before giving up
whisper_timeout_seconds = 30
# Keyboard typing timeout in seconds
# How long to wait for virtual keyboard to finish typing text before giving up
keyboard_timeout_seconds = 5
# Socket connection timeout in seconds
# How long to wait for client to connect to daemon's Unix socket
socket_connect_timeout_seconds = 5
# Socket operation timeout in seconds
# How long to wait for socket operations (send/receive) to complete
socket_operation_timeout_seconds = 10
# Model download timeout in seconds
# How long to wait for Whisper model download to complete before giving up
model_download_timeout_seconds = 300
